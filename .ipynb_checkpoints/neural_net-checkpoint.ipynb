{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually defining the stopwords\n",
    "stop_words = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"could\",\"did\",\"do\",\"does\",\"doing\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"has\",\"have\",\"having\",\"he\",\"he’d\",\"he’ll\",\"he’s\",\"her\",\"here\",\"here’s\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how’s\",\"I\",\"I’d\",\"I’ll\",\"I’m\",\"I’ve\",\"if\",\"in\",\"into\",\"is\",\"it\",\"it’s\",\"its\",\"itself\",\"let’s\",\"me\",\"more\",\"most\",\"my\",\"myself\",\"nor\",\"of\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"she\",\"she’d\",\"she’ll\",\"she’s\",\"should\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that’s\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there’s\",\"these\",\"they\",\"they’d\",\"they’ll\",\"they’re\",\"they’ve\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"we\",\"we’d\",\"we’ll\",\"we’re\",\"we’ve\",\"were\",\"what\",\"what’s\",\"when\",\"when’s\",\"where\",\"where’s\",\"which\",\"while\",\"who\",\"who’s\",\"whom\",\"why\",\"why’s\",\"with\",\"would\",\"you\",\"you’d\",\"you’ll\",\"you’re\",\"you’ve\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n",
    "number_of_classes = 2 # love and sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing part\n",
    "def data_preprocessing(train):\n",
    "    x_train = train.loc[:,['Document']]\n",
    "    y_train = train.loc[:,['Sentiment']]\n",
    "    y_train = y_train['Sentiment'].astype('category')\n",
    "    y_train.cat.categories = range(y_train.nunique())\n",
    "    y_train = np.ravel(y_train)\n",
    "    x_train = list(x_train.Document)\n",
    "    \n",
    "    return (x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data):\n",
    "    words_corpus = set()\n",
    "    for row in data:\n",
    "        row = re.sub(r'^[a-z][A-z]', '', row)\n",
    "        row = row.split()\n",
    "        row = [word for word in row if not word in stop_words]\n",
    "        for word in row:\n",
    "            words_corpus.add(word)\n",
    "    \n",
    "    words_corpus = list(words_corpus)\n",
    "    return words_corpus\n",
    "\n",
    "def count_vectorize(data,words_corpus):\n",
    "    \n",
    "    word_count = [[0]*len(words_corpus) for i in range(len(data))]\n",
    "    i=0\n",
    "    for row in data:\n",
    "        row = re.sub(r'^[a-z][A-z]', '', row)\n",
    "        row = row.split()\n",
    "        row = [word for word in row if not word in stop_words]\n",
    "        for word in row:\n",
    "            if not word in stop_words:\n",
    "                word_count[i][words_corpus.index(word)] += 1\n",
    "        i+=1\n",
    "  \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    one_hot_data = []\n",
    "    for x in data:\n",
    "        temp = np.zeros(number_of_classes)\n",
    "        temp[x]=1\n",
    "        one_hot_data.append(temp)\n",
    "    return one_hot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(x_train,y_train):\n",
    "    \n",
    "    epoch = 50\n",
    "    batch_size = 20\n",
    "    learning_rate = 0.01\n",
    "    seed = 100\n",
    "    \n",
    "    input_neurons = len(x_train[0])\n",
    "    hidden_neurons = 100\n",
    "    output_neurons = number_of_classes\n",
    "    x = tf.placeholder(tf.float32,[None,input_neurons])\n",
    "    y = tf.placeholder(tf.float32,[None,2])\n",
    "    \n",
    "    weights = {'hidden':tf.Variable(tf.random_normal([input_neurons,hidden_neurons],seed=seed)),\n",
    "               'output':tf.Variable(tf.random_normal([hidden_neurons,output_neurons],seed=seed))}\n",
    "    biases  = {'hidden':tf.Variable(tf.random_normal([hidden_neurons],seed=seed)),\n",
    "               'output':tf.Variable(tf.random_normal([output_neurons],seed=seed))}\n",
    "    \n",
    "    hidden_layer = tf.add(tf.matmul(x,weights['hidden']),biases['hidden'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    \n",
    "    output_layer = tf.add(tf.matmul(hidden_layer,weights['output']),biases['output'])\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=output_layer))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the test sentence : my name is ashutosh\n"
     ]
    }
   ],
   "source": [
    "def main(test):\n",
    "    train = pd.read_csv('my_dataset.csv')\n",
    "    # print(train.describe())\n",
    "    x_train,y_train = data_preprocessing(train)\n",
    "    \n",
    "    # creating the word corpus using both test and train data\n",
    "    x_train.extend(test)\n",
    "    words_corpus = create_corpus(x_train)\n",
    "    x_train.pop(-1)\n",
    "    \n",
    "    # creating count vectors\n",
    "    x_train = count_vectorize(x_train,words_corpus)\n",
    "    x_test = count_vectorize(test,words_corpus)\n",
    "    \n",
    "    # one hot encoding of the test data\n",
    "    # for example: [0] -> [1,0] and [1] -> [0,1]\n",
    "    y_train = one_hot(y_train)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    test_sentence = []\n",
    "    test_sentence.append(raw_input('Enter the test sentence : '))\n",
    "    # calling the main function\n",
    "    main(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[1,2]\n",
    "y=x.pop(-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
